{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating a Regression Model Performance\n",
    "\n",
    "### R-Squared\n",
    "\n",
    "The coefficient of determination\n",
    "\n",
    "If $y_{av}$ is the mean of the observed data: $$y_{av}=\\frac{1}{n}\\sum_{i=1}^{n}y_{i}$$\n",
    "\n",
    "Then the total sum of squares $SS_{tot}$ is the sum of the distances between the induvidual data points and the line of the average $y_{av}$: $$SS_{tot}=\\sum_{i}(y_{i}-y_{av})^2$$\n",
    "\n",
    "The residual (regression) sum of squares, also known as the explained sum of sqaures), $SS_{res}$ is the sum of the distances between the induvidual data points and the line of regression: $$SS_{res}=\\sum_{i}(y_{i}-y_{regression})^2$$\n",
    "\n",
    "$R^{2}$ is thus: $$R^{2} = 1 - \\frac{SS_{res}}{SS_{tot}}$$\n",
    "\n",
    "It basically a comparison between the average line and the regresison line.\n",
    "\n",
    "Closer r-squared is to 1, the better.\n",
    "\n",
    "### Adjusted R-squared\n",
    "\n",
    "When you have a model with multiple variables (multiple coefficients), need an adjusted R-squared. $$R_{adj}^{2}=1-(1-R^{2})\\frac{n-1}{n-p-1}$$\n",
    "\n",
    "where p is the number of regressor and n is the sample size.\n",
    "\n",
    "Will look back over the backwards elimination/multiple linear regression method.\n",
    "\n",
    "Earlier, we removed the Marketing spend because the p value was 0.06 (>0.05) and that seemed a little strange as the chosen p values are abitrary.\n",
    "\n",
    "So check if it was correct to remove the independent variable (marketing spend) you can check how it affects the adj r-squared. Did it go up or down?\n",
    "\n",
    "In the case of removing marketing spend, the adj. r-squared went down. So it wasn't correct to remove it.\n",
    "\n",
    "### Choosing a Model\n",
    "\n",
    "#### Linear Regression\n",
    "PRO - Works on any size dataset, gives information about relevance of features\n",
    "CON - Linear regression assumptions\n",
    "\n",
    "#### Polynomial Regression\n",
    "PRO - Works on any size dataset, works very well on non-linear problems\n",
    "CON - Need to choose right polynomial degree for good bias/variance tradeoff\n",
    "\n",
    "#### SVR\n",
    "PRO - Easily adaptable, works very well on non-linear problems, not biased by outliers\n",
    "CON - Compulsory to apply feature scaling, more difficult to understand\n",
    "\n",
    "#### Decision Tree Regression\n",
    "PRO - Interpretability, no need for feature scaling, works on both linear/non-linear problems\n",
    "CON - Poor results on small datasets, overfitting easily occurs\n",
    "\n",
    "#### Random Forest Regression\n",
    "PRO - Powerful and accurate, good performance on many problems including non-linear\n",
    "CON - No interpretability, overfitting can easily occur, need to choose number of trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
